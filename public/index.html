<!DOCTYPE html>
<html lang="en">
  <html>
    <head>
      <meta charset="UTF-8" />
      <meta http-equiv="X-UA-Compatible" content="IE=edge" />
      <meta name="viewport" content="width=device-width, initial-scale0" />
      <title>Voice recorder</title>
      <style>
        * {
          margin: 0px;
          padding: 0;
          box-sizing: border-box;
        }
        body {
          padding: 30px;
          font-family: Arial, Helvetica, sans-serif;
          text-align: center;
          margin-top: 20%;
        }
        button {
          padding: 7px 20px;
          cursor: pointer;
        }

        #response {
          background-color: #e0e0e0;
          border-radius: 8px;
          margin-top: 20px;
          padding: 12px;
          width: fit-content;
          margin-left: auto;
          margin-right: auto;
        }
      </style>
      <script src="https://cdn.jsdelivr.net/npm/axios@1.6.2/dist/axios.min.js"></script>
      <script src="https://www.WebRTC-Experiment.com/RecordRTC.js"></script>
      <script
        src="https://cdn.socket.io/4.7.2/socket.io.min.js"
        integrity="sha384-mZLF4UVrpi/QTWPA7BjNPEnkIfRFn4ZEO3Qt/HFklTJBj/gBOV8G3HcKn4NfQblz"
        crossorigin="anonymous"
      ></script>
    </head>
    <body>
      <button id="startRecording">Start</button>
      <button id="stopRecording">Stop</button>
      <br />
      <p id="isRecording" style="margin-top: 10px">
        Click start button to record
      </p>
      <br />
      <audio
        src="http://localhost:3000/uploads/user-socket-input.wav"
        id="audioElement"
        controls
      ></audio>

      <div id="response">No Transcription Available</div>

      <script>
        let socket;
        let transcribedData = "";
        let formData;
        let recordAudio;
        let hasInitialResponse = false;
        let streamEndedCounter = 0;
        let startTime,
          timeTaken,
          endTime = null;
        let stopEventCalled = false;
        let startEventCalled = false;
        let fetchedResponse = [];
        let audioBuffers = [];
        const DONE = "[DONE]";
        let streamEnded = false;
        let firstAudioPlayCall = false;

        function createSocketConnection() {
          if (!socket) {
            socket = io.connect("http://localhost:3000");
            console.log("Connection established!");
          }
        }
        createSocketConnection();

        document
          .getElementById("startRecording")
          .addEventListener("click", initFunction, false);
        let isRecording = document.getElementById("isRecording");

        function initFunction() {
          const synth = window.speechSynthesis;
          stopEventCalled = false;
          startEventCalled = true;
          streamEnded = false;
          firstAudioPlayCall = false;
          time = 0;

          function speak(content) {
            const utterThis = new SpeechSynthesisUtterance(content);
            synth.speak(utterThis);
          }

          function getResponse(text) {
            axios
              .post("http://localhost:3000/respond", { text })
              .then(function (response) {
                console.log(response);
              })
              .catch(function (err) {
                console.log(err);
              });
          }

          function createBlob(blob, isLast = false) {
            socket.emit("microphone", blob);
          }

          async function transcribeAudio(formData, isLast = false) {
            if (isLast) {
              formData.append("lastChunk", true);
            }

            const response = await fetch(`http://localhost:3000/upload-v2`, {
              method: "POST",
              body: formData,
            });

            let chatMessage = "";
            let reader = response.body
              ?.pipeThrough(new TextDecoderStream())
              .getReader();

            while (reader) {
              let stream = await reader.read();

              if (stream.done) {
                endTime = new Date().getTime();
                timeTaken = (endTime - startTime) / 1000;
                break;
              }

              const chunks = stream.value
                .replaceAll(/^data: /gm, "")
                .split("\n")
                .filter((c) => Boolean(c.length) && c !== "[DONE]")
                .map((c) => JSON.parse(c));

              if (chunks) {
                for (let chunk of chunks) {
                  const content = chunk.choices[0].delta.content;

                  if (!content) continue;
                  chatMessage += content;

                  if (!hasInitialResponse) {
                    document.getElementById("response").innerHTML = "";
                    hasInitialResponse = true;
                  }

                  document.getElementById("response").innerHTML =
                    getFormattedTranscribedData(chatMessage, timeTaken);
                }
              }
            }

            document.getElementById("response").innerHTML =
              getFormattedTranscribedData(chatMessage, timeTaken);

            console.log("Fetching speaking tokens ...");
            const responseData = await fetch(`http://localhost:3000/speak`, {
              method: "POST",
              body: JSON.stringify({ text: chatMessage }),
              headers: {
                "Content-Type": "application/json",
              },
            });

            if (responseData) {
              document.getElementById("audioElement").src =
                "http://localhost:3000/uploads/output.mp3";
              document.getElementById("audioElement").play();
            }
          }

          async function getUserMedia(constraints) {
            if (window.navigator.mediaDevices) {
              return window.navigator.mediaDevices.getUserMedia(constraints);
            }
            let legacyApi =
              navigator.getUserMedia ||
              navigator.webkitGetUserMedia ||
              navigator.mozGetUserMedia ||
              navigator.msGetUserMedia;
            if (legacyApi) {
              return new Promise(function (resolve, reject) {
                legacyApi.bind(window.navigator)(constraints, resolve, reject);
              });
            } else {
              alert("user api not supported");
            }
          }

          isRecording.textContent = "Recording...";

          function handlerFunction(stream) {
            recordAudio = RecordRTC(stream, {
              type: "audio",
              mimeType: "audio/wav",
              sampleRate: 44100,
              desiredSampRate: 16000,
              timeSlice: 3500,
              disableLogs: true,
              recorderType: StereoAudioRecorder,
              numberOfAudioChannels: 1,
              ondataavailable: function (blob) {
                console.log("Blob created...");
                createBlob(blob);
              },
            });
            recordAudio.startRecording();
          }

          function startusingBrowserMicrophone(boolean) {
            getUserMedia({ audio: boolean }).then((stream) => {
              handlerFunction(stream);
            });
          }

          startusingBrowserMicrophone(true);

          function stopRecording(event) {
            if (!stopEventCalled) {
              console.log("Click");
              recordAudio.stopRecording(function (blob) {
                startTime = new Date().getTime();
              });

              isRecording.textContent = "Click play button to start listening";
              socket.emit("microphone", "[DONE]");
              stopEventCalled = true;
              startEventCalled = false;
              hasInitialResponse = false;
            }
          }

          function recordingEnded(event) {
            if (
              audioBuffers.length > 0 &&
              document.getElementById("audioElement").ended
            ) {
              playAudio();
            }
          }

          document
            .getElementById("stopRecording")
            .addEventListener("click", stopRecording, false);

          document
            .getElementById("audioElement")
            .addEventListener("ended", recordingEnded, false);
        }

        function getFormattedTranscribedText(data, time) {
          return `<i>${data}</i>`;
        }

        socket.on("transcription", function (data) {
          console.log(`Received Transcription... ${data}`);
          document.getElementById("response").innerHTML = "";
        });

        let context = new AudioContext();
        let time = 0;
        let source;

        socket.on("audio", function (buffer) {
          //audioBuffers.push(buffer);
          //playSound(buffer);
          context.decodeAudioData(buffer, function(soundBuffer){
            stream(soundBuffer)
          });
        });

        function stream(buffer){
            source = context.createBufferSource();
            source.buffer = buffer;
            source.connect(context.destination);
            source.loop = false;

            let contextWindow = (context.currentTime - 1.3) <= 0? 0: (context.currentTime - 1.3);
            source.start(time + contextWindow);
            time += buffer.duration
        }
        
        function playAudio() {
          if (audioBuffers.length > 0) {
            let buffer = audioBuffers.shift();
            document.getElementById("audioElement").src = URL.createObjectURL(
              new Blob([buffer])
            );
            document.getElementById("audioElement").play();
          }
        }

        socket.on("response", function (data) {
          if (data === DONE) {
            streamEnded = true;
          } else {
            endTime = new Date().getTime();
            timeTaken = (endTime - startTime) / 1000;
            console.log(`Response Received In ${timeTaken}`);
            document.getElementById("response").innerHTML +=
              getFormattedTranscribedText(data);
            console.log(`Response - ${data}`);
          }
        });
      </script>
    </body>
  </html>
</html>
